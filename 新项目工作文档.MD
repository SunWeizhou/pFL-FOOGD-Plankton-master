Project Specification: pFL-FOOGD for Marine Plankton Recognition

项目规格说明书：基于 pFL-FOOGD 的海洋浮游生物图像识别

1. Project Overview (项目概述)

This project aims to implement a Federated Learning (FL) framework for marine plankton image recognition. The framework, named pFL-FOOGD, integrates FedRoD (for handling Non-IID data) with FOOGD (for Out-of-Distribution generalization and detection).

Key Objectives:

Backbone: DenseNet-169 (or DenseNet-121 if memory constrained).

FL Strategy: FedRoD (Federated Robust Decoupling).

OOD Strategy:

SAG (Stein Augmented Generalization): Using KSD Loss for domain generalization.

SM3D (Score Matching with MMD): Training a score model for OOD detection.

Dataset: DYB-PlanktonNet (Split based on Han et al. Benchmark).

2. Dataset & Directory Structure (数据与目录)

2.1 Directory Structure (预期目录结构)

The code should assume the following data layout:

./data/
    ├── ID_images/           # 54 Classes (Train/Val/Test)
    ├── OOD_Near/            # 26 Classes (Test Only)
    └── OOD_Far/             # 12 Classes (Bubbles & Particles, Test Only)


2.2 Class Definitions (严格类别定义)

CRITICAL: The dataset must be split strictly according to these lists.

[A] ID Classes (54 Classes) - Target Labels (0-53)
Used for Training (80%), Validation (10%), Test (10%)

ID_CLASSES = [
    "Polychaeta_most with eggs", "Polychaeta Type A", "Polychaeta Type B", "Polychaeta Type C",
    "Polychaeta Type D", "Polychaeta Type E", "Polychaeta Type F", "Penilia avirostris",
    "Evadne tergestina", "Acartia sp.A", "Acartia sp.B", "Acartia sp.C", "Calanopia sp.",
    "Labidocera sp.", "Tortanus gracilis", "Calanoid with egg", "Calanoid Type A",
    "Calanoid Type B", "Oithona sp.B with egg", "Cyclopoid Type A with egg",
    "Harpacticoid mating", "Microsetella sp.", "Caligus sp.", "Copepod Type A", "Caprella sp.",
    "Amphipoda Type A", "Amphipoda Type B", "Amphipoda Type C", "Gammarids Type A",
    "Gammarids Type B", "Gammarids Type C", "Cymodoce sp.", "Lucifer sp.", "Macrura larvae",
    "Megalopa larva Phase 1 Type B", "Megalopa larva Phase 1 Type C",
    "Megalopa larva Phase 1 Type D", "Megalopa larva_Phase 2", "Porcrellanidae larva",
    "Shrimp-like larva Type A", "Shrimp-like larva Type B", "Shrimp-like Type A",
    "Shrimp-like Type B", "Shrimp-like Type D", "Shrimp-like Type F", "Cumacea Type A",
    "Cumacea Type B", "Chaetognatha", "Oikopleura sp. parts", "Tunicata Type A",
    "Jellyfish", "Creseis acicula", "Noctiluca scintillans", "Phaeocystis globosa"
]


[B] Near-OoD Classes (26 Classes)
Used ONLY for Testing (OOD Evaluation)

NEAR_OOD_CLASSES = [
    "Polychaeta larva", "Calanoid Nauplii", "Calanoid Type C", "Calanoid Type D",
    "Oithona sp.A with egg", "Cyclopoid Type A", "Harpacticoid", "Monstrilla sp.A",
    "Monstrilla sp.B", "Megalopa larva Phase 1 Type A", "Shrimp-like Type C",
    "Shrimp-like Type E", "Ostracoda", "Oikopleura sp.", "Actiniaria larva", "Hydroid",
    "Jelly-like", "Bryozoan larva", "Gelatinous Zooplankton", "Unknown Type A",
    "Unknown Type B", "Unknown Type C", "Unknown Type D", "Balanomorpha exuviate",
    "Monstrilloid", "Fish Larvae"
]


[C] Far-OoD Classes (12 Classes)
Used ONLY for Testing (OOD Evaluation)

FAR_OOD_CLASSES = [
    "Crustacean limb Type A", "Crustacean limb_Type B", "Fish egg",
    "Particle filamentous Type A", "Particle filamentous Type B", "Particle bluish",
    "Particle molts", "Particle translucent flocs", "Particle_yellowish flocs",
    "Particle_yellowish rods", "Bubbles", "Fish tail"
]


3. Architecture Design (架构设计)

3.1 Backbone Network

Model: DenseNet-169 (Pre-trained on ImageNet).

Feature Dimension: Extract features before the final classification layer.

3.2 FedRoD Implementation (Federated Robust Decoupling)

FedRoD requires two classification heads on top of the backbone:

Generic Head (Head_G): Aggregated by the server. Learns common knowledge.

Personalized Head (Head_P): Kept local (no aggregation). Learns client-specific bias.

Training Logic:

For input $x$, generate features $z = f(x)$.

Compute $L_{total} = L_{CE}(Head\_G(z), y) + L_{CE}(Head\_P(z), y)$.

The Server only aggregates the Backbone and Head_G.

3.3 FOOGD Integration (OOD Modules)

FOOGD is applied to the Generic Head/Features.

A. SAG (Stein Augmented Generalization) - for OOD Generalization

Mechanism: Apply Kernelized Stein Discrepancy (KSD) Loss on the features $z$.

Goal: Regularize the feature space to handle domain shifts (water turbidity, etc.).

B. SM3D (Score Matching) - for OOD Detection

Mechanism: Train a separate lightweight "Score Model" (e.g., a small MLP or ResNet block) $s_\theta(z)$ to estimate the gradient of the log-density of the features: $\nabla_z \log p(z)$.

Inference:

Score = $\|s_\theta(z)\|$.

High score -> High density (ID).

Low score -> Low density (OOD).

4. Federated Simulation Setup (联邦模拟设置)

Total Clients: $K=10$.

Data Partition: Non-IID partition using Dirichlet Distribution ($\alpha$).

Scenario 1: $\alpha = 0.1$ (High Heterogeneity - Primary Experiment).

Scenario 2: $\alpha = 5.0$ (Low Heterogeneity).

Communication Rounds: 50-100 rounds (to be tuned).

Local Epochs: 1-5 epochs per round.

5. Required Code Modules (编码任务清单)

Please implement the following modules in Python (PyTorch):

Step 1: Data Preparation

data_utils.py:

PlanktonDataset class (Resize to 224x224, Transforms).

partition_data(dataset, n_clients, alpha): Function to split data indices using Dirichlet distribution.

Step 2: Models

models.py:

Backbone: DenseNet-169 feature extractor.

FedRoD_Model: A wrapper containing Backbone, Head_G, and Head_P.

ScoreModel: A small network for SM3D.

Step 3: Federated Logic

client.py:

train_step(): Compute FedRoD Loss + KSD Loss + Score Matching Loss.

get_parameters(): Return Generic parameters.

server.py:

aggregate(): FedAvg logic for Generic parameters.

Step 4: Evaluation

eval_utils.py:

evaluate_id(): Accuracy on ID test set.

evaluate_ood(): Input ID test set (label=1) vs OOD sets (label=0). Compute AUROC and FPR95.

6. Environment Constraints (环境限制)

GPU: Single NVIDIA RTX 3060 (6GB VRAM).

Memory Optimization:

Use batch_size=16 or 32.

Use Mixed Precision (torch.cuda.amp).

If DenseNet-169 causes OOM, provide a fallback switch to DenseNet-121.